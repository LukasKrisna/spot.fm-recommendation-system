# -*- coding: utf-8 -*-
"""Submission 2_Recommendation System_Lukas Krisna.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10mzCjO2iUbaIWcMKptOgDh5Um0xOmIkh
"""

!pip install tensorflow

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from google.colab import drive
drive.mount('/content/drive')

music_info = "/content/drive/MyDrive/Music Info.csv"
user_listening_history = "/content/drive/MyDrive/User Listening History.csv"

music_info_df = pd.read_csv(music_info)
user_history_df = pd.read_csv(user_listening_history)

print('Original User Listening History shape:', user_history_df.shape)
print('Original Music Info shape:', music_info_df.shape)

"""## Exploratory Data Analysis"""

music_info_df.info()

user_history_df.info()

user_history_df.count()

user_history_df_reduced = user_history_df.sample(n=100000, random_state=42)

user_history_df_reduced.count()

print('\nUnique users:', user_history_df_reduced['user_id'].nunique())
print('Unique tracks listened:', user_history_df_reduced['track_id'].nunique())
print('Playcount statistics:\n', user_history_df_reduced['playcount'].describe())

user_history_df_reduced = user_history_df_reduced[user_history_df_reduced['playcount'] <= 6]

plt.figure(figsize=(10, 6))
user_history_df_reduced['playcount'].plot(kind='hist', bins=5, title='Distribution of Playcounts')
plt.xlabel('Playcount')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

music_info_df.info()
print('\nUnique tracks:', music_info_df['track_id'].nunique())
print('Unique artists:', music_info_df['artist'].nunique())

music_info_df.tags.head()

# Clean and extract the first tag as genre
music_info_df['tags_cleaned'] = music_info_df['tags'].apply(lambda x: x.split(',')[0].strip() if pd.notna(x) and x.strip() else np.nan)
# Further clean the genre by removing any non-alphabetic characters
music_info_df['tags_cleaned'] = music_info_df['tags_cleaned'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', str(x)).strip().lower() if pd.notna(x) else np.nan)

music_info_df.tags_cleaned

music_info_df.tags_cleaned.unique()

# Removing NaN
music_info_df.dropna(subset=['tags_cleaned'], inplace=True)

music_info_df.tags_cleaned

print('\nUnique raw tags (first 20):', music_info_df['tags_cleaned'].unique()[:20])

plt.figure(figsize=(12, 7))
music_info_df['tags_cleaned'].value_counts().head(15).plot(kind='bar', title='Top 15 Genres')
plt.xlabel('Genre')
plt.ylabel('Number of Tracks')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 7))
music_info_df['artist'].value_counts().head(15).plot(kind='bar', title='Top 15 Artists')
plt.xlabel('Artist')
plt.ylabel('Number of Tracks')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""## Data Preparation"""

# Merge the datasets for collaborative filtering later
# We'll use a clean_music_df for content-based first.
# For collaborative filtering, we need user_id, track_id, and playcount.
df_cf = user_history_df_reduced.copy()
print(df_cf.shape)

df_cf

clean_music_df = music_info_df[['track_id', 'name', 'artist', 'tags_cleaned']].copy()
clean_music_df.rename(columns={'tags_cleaned': 'genre'}, inplace=True)
print('\nClean Music Info for Content-Based Filtering shape:', clean_music_df.shape)
print('Missing values in clean_music_df:\n', clean_music_df.isnull().sum())

clean_music_df.dropna(subset=['name', 'artist', 'genre'], inplace=True)
print('\nClean Music Info after dropping NaNs shape:', clean_music_df.shape)

clean_music_df = clean_music_df.drop_duplicates('track_id')
print('\nClean Music Info after dropping duplicate track_ids shape:', clean_music_df.shape)

# Convert track_id, name, artist and genre to lists
track_ids_cb = clean_music_df['track_id'].tolist()
track_names_cb = clean_music_df['name'].tolist()
track_artist_cb = clean_music_df['artist'].tolist()
track_genres_cb = clean_music_df['genre'].tolist()

print(f'Number of unique track IDs for CB: {len(track_ids_cb)}')
print(f'Number of unique track names for CB: {len(track_names_cb)}')
print(f'Number of unique track names for CB: {len(track_artist_cb)}')
print(f'Number of unique track genres for CB: {len(track_genres_cb)}')

# Create a new DataFrame for content-based filtering with unique tracks
music_for_content_based = pd.DataFrame({
    'track_id': track_ids_cb,
    'track_name': track_names_cb,
    'artist': track_artist_cb,
    'genre': track_genres_cb
})
print('\nMusic for Content-Based Filtering DataFrame:\n', music_for_content_based.head())

"""## Model Development with Content Based Filtering"""

data_cb = music_for_content_based
data_cb.sample(5)

"""### TF-IDF Vectorizer"""

tfidf_vectorizer = TfidfVectorizer()

tfidf_vectorizer.fit(data_cb['genre'])

print('\nTF-IDF Features:\n', tfidf_vectorizer.get_feature_names_out())

tfidf_matrix = tfidf_vectorizer.fit_transform(data_cb['genre'])
print('\nTF-IDF Matrix Shape:', tfidf_matrix.shape)

# Vector tf-idf to matrix
tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf_vectorizer.get_feature_names_out(),
    index=data_cb.track_name
).sample(10, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

# Calculate cosine similarity on matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
print('\nCosine Similarity Matrix Shape:', cosine_sim.shape)

# Create a dataframe from the cosine_sim variable with rows and columns of track names
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_cb['track_name'], columns=data_cb['track_name'])
print('Cosine Similarity DataFrame Shape:', cosine_sim_df.shape)

print('\nSample of Cosine Similarity Matrix:\n', cosine_sim_df.sample(5, axis=1).sample(10, axis=0))

"""### Recommendation (Content Based)"""

def music_recommendations(song_name, similarity_data=cosine_sim_df, items=data_cb[['track_name', 'artist', 'genre']], k=5):

    if song_name not in similarity_data.columns:
        print(f"Song '{song_name}' not found in the similarity data.")
        return pd.DataFrame()

    # Retrieve data using argpartition to indirectly partition along the given axis
    # The DataFrame is converted to a numpy array
    # Range(start, stop, step)
    index = similarity_data.loc[:,song_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # Get data with the greatest similarity from the existing index
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop the song name so the searched song doesn't appear in the recommendation list
    closest = closest.drop(song_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

music_info_df

"""### Get Recommendation"""

music_recommendations('The Revelation')

music_recommendations('Wonderwall')

"""### Evaluation"""

# ... (previous code for data_cb, tfidf_vectorizer, cosine_sim_df, music_recommendations)

# --- NEW: Sample the tracks for coverage calculation ---
# Sample a smaller number of tracks to evaluate coverage
sample_size = 1000 # Or 500, or 2000, depending on how much time you have and accuracy needed
sampled_tracks_for_coverage = data_cb['track_name'].sample(n=sample_size, random_state=42).tolist()

recommended_unique_tracks = set()

print(f"Calculating coverage based on a sample of {len(sampled_tracks_for_coverage)} tracks...")
for i, track_name in enumerate(sampled_tracks_for_coverage):
    if (i + 1) % 100 == 0: # Print progress every 100 tracks
        print(f"  Processed {i + 1}/{len(sampled_tracks_for_coverage)} tracks...")

    try:
        recs_df = music_recommendations(track_name, k=10) # Get top 10 recommendations
        recommended_unique_tracks.update(recs_df['track_name'].tolist())
    except Exception as e:
        # Handle cases where song_name might not be found (unlikely if sampled from data_cb)
        # print(f"Could not get recommendations for '{track_name}': {e}")
        pass

total_unique_tracks_in_dataset = len(data_cb['track_name'].unique()) # Still compare against the full set
coverage = len(recommended_unique_tracks) / total_unique_tracks_in_dataset

print(f"\nTotal unique tracks in dataset: {total_unique_tracks_in_dataset}")
print(f"Number of unique tracks recommended (from sample): {len(recommended_unique_tracks)}")
print(f"Content-Based System Coverage (Estimated): {coverage:.2%}")

"""## Model Development with Collaborative Filtering"""

df_cf = pd.merge(user_history_df_reduced, music_for_content_based[['track_id', 'track_name', 'artist', 'genre']], on='track_id', how='inner')

df_cf

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df_cf['user_id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah placeID menjadi list tanpa nilai yang sama
track_ids = df_cf['track_id'].unique().tolist()

# Melakukan proses encoding placeID
track_to_track_encoded = {x: i for i, x in enumerate(track_ids)}

# Melakukan proses encoding angka ke placeID
track_encoded_to_track = {i: x for i, x in enumerate(track_ids)}

# Mapping userID ke dataframe user
df_cf['user'] = df_cf['user_id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe resto
df_cf['track'] = df_cf['track_id'].map(track_to_track_encoded)

df_cf

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_track = len(track_encoded_to_track)
print(num_track)

# Mengubah rating menjadi nilai float
df_cf['playcount'] = df_cf['playcount'].values.astype(np.float32)

# Nilai minimum playcount
min_playcount = min(df_cf['playcount'])

# Nilai maksimal playcount
max_playcount = max(df_cf['playcount'])

print('Number of User: {}, Number of Track: {}, Min Playcount: {}, Max Playcount: {}'.format(
    num_users, num_track, min_playcount, max_playcount
))

# Mengacak dataset
df_cf = df_cf.sample(frac=1, random_state=42)
df_cf

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df_cf[['user', 'track']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df_cf['playcount'].apply(lambda x: (x - min_playcount) / (max_playcount - min_playcount)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.7 * df_cf.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_track, embedding_size, dropout_rate=0.2, **kwargs): # Add dropout_rate parameter
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_track = num_track
    self.embedding_size = embedding_size
    self.dropout_rate = dropout_rate # Store dropout rate

    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(5e-4)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.track_embedding = layers.Embedding( # layer embeddings track
        num_track,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(5e-4)
    )
    self.track_bias = layers.Embedding(num_track, 1) # layer embedding track bias

    # Add Dropout layers
    self.user_dropout = layers.Dropout(self.dropout_rate)
    self.track_dropout = layers.Dropout(self.dropout_rate)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    track_vector = self.track_embedding(inputs[:, 1])
    track_bias = self.track_bias(inputs[:, 1])

    # Apply dropout to the embedding vectors
    user_vector = self.user_dropout(user_vector)
    track_vector = self.track_dropout(track_vector)

    dot_user_track = tf.tensordot(user_vector, track_vector, 2)

    x = dot_user_track + user_bias + track_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_track, embedding_size=5, dropout_rate=0.2) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0005),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_root_mean_squared_error', # Monitor the validation RMSE
    patience=3,                           # Number of epochs with no improvement after which training will be stopped.
    restore_best_weights=True             # Restore model weights from the epoch with the best value of the monitored quantity.
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 50,
    validation_data = (x_val, y_val),
    callbacks=[early_stopping_callback]
)

"""### Get Recommendation"""

music_df = music_for_content_based
# Changed to sample user from df_cf instead of user_history_df
df_for_sampling = df_cf # Use df_cf for sampling users and finding top tracks

# Mengambil sample user
# Sample from df_cf to ensure the user is in the training data
user_id = df_for_sampling.user_id.sample(1).iloc[0]
track_played_by_user = df_for_sampling[df_for_sampling.user_id == user_id] # Use df_cf for user's history

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
track_not_played_ids = music_df[~music_df['track_id'].isin(track_played_by_user.track_id.values)]['track_id']

# Filter out track_ids that are not in the encoded map
track_not_played_encoded = [
    [track_to_track_encoded.get(x)] for x in track_not_played_ids
    if x in track_to_track_encoded # Only include if track_id exists in the encoded map
]

# Get the encoded user_id, check if it exists
user_encoder = user_to_user_encoded.get(user_id)

# Check if user_id was successfully encoded and there are tracks to recommend
if user_encoder is not None and len(track_not_played_encoded) > 0:
    user_track_array = np.hstack(
        ([[user_encoder]] * len(track_not_played_encoded), track_not_played_encoded)
    )

    # The rest of the recommendation logic should be inside this if block
    # to avoid calling predict on an empty or invalid array
    tracks = model.predict(user_track_array).flatten()

    top_tracks_indices = tracks.argsort()[-10:][::-1]
    recommended_track_ids = [
        track_encoded_to_track.get(track_not_played_encoded[x][0]) for x in top_tracks_indices
    ]

    print('Showing recommendations for users: {}'.format(user_id))
    print('===' * 9)
    print('Tracks with most played from user')
    print('----' * 8)

    top_track_user = (
        track_played_by_user.sort_values( # Use track_played_by_user which is filtered from df_cf
            by = 'playcount',
            ascending=False
        )
        .head(5)
        .track_id.values
    )

    track_df_rows = music_df[music_df['track_id'].isin(top_track_user)]
    for row in track_df_rows.itertuples():
        print(row.track_name, ':', row.genre)

    print('----' * 8)
    print('Top 10 track recommendation') # Corrected the print statement
    print('----' * 8)

    recommended_track = music_df[music_df['track_id'].isin(recommended_track_ids)]
    for row in recommended_track.itertuples():
        print(row.track_name, ':', row.genre)
else:
    print(f"Could not generate recommendations for user {user_id}. Either user not found in encoded list or no unplayed tracks available.")

"""### Evaluation"""

plt.figure(figsize=(10, 6))
plt.plot(history.history['root_mean_squared_error'], label='train_rmse')
plt.plot(history.history['val_root_mean_squared_error'], label='val_rmse')
plt.title('Model Metrics (Collaborative Filtering)')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(loc='upper left')
plt.grid(True)
plt.show()

